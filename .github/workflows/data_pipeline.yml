name: Genomic Data Pipeline

on:
  push:
    branches:
      - master  # Trigger on any commit to the 'main' branch
  pull_request:
    branches:
      - master  # Trigger on pull requests to the 'main' branch
  schedule:
    - cron: '30 18 * * *'  # Runs daily at midnight IST (18:30 UTC)

jobs:
  generate_and_process_data:
    runs-on: ubuntu-latest
    
    steps:
      # Checkout the repository
      - name: Checkout code
        uses: actions/checkout@v3

      # Set up Python
      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: '3.8'

      # Install dependencies from requirements.txt
      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      # Step 1: Generate real-time genomic data
      - name: Generate real-time genomic data
        run: |
          python data_generation.py

      # Step 2: Process and clean the data
      - name: Process and clean the data
        run: |
          python data_processing.py

      # Step 3: List files to confirm cleaned data exists
      - name: List files to confirm cleaned data exists
        run: |
          ls -alh  # List files to debug and confirm if cleaned data exists

      # Step 4: Upload cleaned genomic data to S3
      - name: Upload cleaned genomic data to AWS S3
        run: |
          aws s3 cp cleaned_real_time_genomic_data.csv s3://genomicbucket/cleaned_data/

  analysis_and_visualization:
    needs: generate_and_process_data
    runs-on: ubuntu-latest

    steps:
      # Checkout the repository again
      - name: Checkout code
        uses: actions/checkout@v3

      # Set up Python
      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: '3.8'

      # Install dependencies from requirements.txt
      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      # Step 1: Download the cleaned genomic data artifact
      - name: Download cleaned genomic data artifact
        uses: actions/download-artifact@v3
        with:
          name: cleaned-genomic-data
          path: ./  # Download to the current directory

      # Step 2: Run analysis
      - name: Run data analysis
        run: |
          python analysis.py

      # Step 3: Run visualizations
      - name: Run visualizations
        run: |
          python visualize.py
          
      # Step 4: Upload the visualization results (e.g., saved as .png, .pdf, etc.)
      - name: Upload visualizations as artifact
        uses: actions/upload-artifact@v3
        with:
          name: visualizations
          path: ./visualizations/*.png  # Assuming visualizations are saved as PNG files

  export_to_aws:
    needs: analysis_and_visualization
    runs-on: ubuntu-latest

    steps:
      # Checkout the repository
      - name: Checkout code
        uses: actions/checkout@v3

      # Set up AWS credentials to export data to S3
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{secrets.AWS_ACCESS_KEY_ID}}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-south-1  # Update with your preferred AWS region

      # Step 1: List files to confirm the cleaned data exists
      - name: List files to confirm cleaned data exists
        run: |
          ls -alh  # List files to confirm if cleaned data exists

      # Step 2: Upload cleaned genomic data to AWS S3
      - name: Upload cleaned genomic data to AWS S3
        run: |
          aws s3 cp cleaned_real_time_genomic_data.csv s3://genomicbucket/cleaned_data/

      # Step 3: Upload visualizations to AWS S3
      - name: Upload visualizations to AWS S3
        run: |
          aws s3 cp ./visualizations/ s3://genomicbucket/visualizations/ --recursive
